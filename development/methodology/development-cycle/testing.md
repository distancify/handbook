When we start developing a PBI we should create a test task for each scenario of the acceptance criteria. More detailed information can be added to the test scenarios, like information about what should be logged, or added test data to cover more edge cases. This can be done by the developer and/or someone specialized in testing. 

The developer then design a solution that will allow to implement the desired functionality, often by breaking it down into different methods. Before implementing the methods it is a good idea to write unit tests for them. If it is difficult to write the unit tests, or if they take long to execute, that may be an indication that the design is not very good. The developer alone is responsible for writing unit tests and making sure that they are execute (and pass) as part of the build process. The developer should also make sure that the acceptance scenarios are fulfilled, if possible by running end-to-end tests. If it is not possible to run full end-to-end tests in the development environment, there should be a feature toggle that makes it possible to turn the functionality on and off.

In parallel with the code review, the developer should have a short handover to the tester where the functionality is demonstrated, i.e. showing how the feature work and not how it is implemented.
The tester setup the QA-environment and makes sure that test data is available and either document each step of the acceptance scenarios using screen dumps and/or detailed descriptions. Alternatively the end-to-end tests can be automated using Cucumber/Specflow in which case the tests themselves can be seen as documentation. 

Not all end-to-end tests should be automated, but they should be automated if the test is deemed to be valuable as a regression test, if we need to test with lots of different test data, if we need to run the test on multiple browser etc., or we already have automated many of the steps in the scenario so test automation is quicker than doing the test manually.

The tester is also expected to do some exploratory testing to see how the feature behaves outside its normal use. This is normally not documented unless some unexpected behavior occurs. When something unexpected happens it should be documented and more time should be put into exploratory testing of that specific feature since bugs often cluster.

When testing is done there should be a handover to the customer where the feature is demoed and the test results are shown. The customer can optionally run their own UAT where they test the feature in the QA-system. 
